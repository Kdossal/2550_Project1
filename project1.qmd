---
title: "PHP 2550: Worksheet 2"
subtitle: "Due: September 15th at 11:59pm"
format: pdf
editor: visual
---

## Reading Recap

1.  For each of the listed readings below, recap the reading in three points. Each point could be summarizing a key takeaway, something that surprised you, or something that you want to remember.

    -   Chapters 1-2 of the "The 9 Pitfalls of Data Science"

    -   [Ten Simple Rules for Reproducible Computational Research](https://canvas.brown.edu/courses/1092384/files/69227767?wrap=1)

    -   [Researcher Requests for Inappropriate Analysis and Reporting](https://canvas.brown.edu/courses/1092384/files/69227757?wrap=1)

2.  How did the readings relate to each other? (1-2 paragraphs)

## Reproducible Code and Documentation

1.  What makes an analysis reproducible? Consider your answer in three settings: (a) when the data and code is available, (b) the data is available but the code can't be shared, and (c) when neither the data or code are available.

Reproducibility in data analysis relies on transparency and accessibility. When both data and code are available, this involves sharing well-documented code alongside the raw dataset and steps taken to prepare the date. When only the data is available, detailed documentation of analysis is crucial. In situations with no access to either data or code, comprehensive written descriptions of the data cleaning, preparation, and analysis process should be provided.

1.  Read the code in the R script "gerrymander_sim.R" on Canvas in the Code Examples folder. The code uses the file "congressional_election_results_post1948.csv" and is implementing a [simulation test](https://gerrymander.princeton.edu/info/) for gerrymandering. Improve the code to be more reproducible by consider the following guidelines.

    -   Use comments, white space, and variable names to improve the readability of your code.

    -   When possible, wrap code into functions that include [roxygen](https://cran.r-project.org/web/packages/roxygen2/vignettes/rd.html) documentation.

    -   Include all libraries and the random seed (if used) at the top of the document.

    -   Avoid using magic numbers - numbers that are not explained or defined.

2.  Data cleaning and documentation is a key part of the data science pipeline. How we pre-process the data can impact our analysis. Additionally, we should be checking our data for potential quality problems. To demonstrate this point, we have provided two data files from Providence's scooter share program in June 2019. The files are "prov_locations.csv" and "prov_events.csv". The first file contains information about scooter locations where each row is a period of time when a scooter was in a set location. The second file records every event in the system.

    ``` R
    library(tidyverse)
    set.seed(0)

    # Data Source: https://gerrymander.princeton.edu/resources
    raw_data <- read.csv("Week 1/congressional_election_results_post1948.csv")

    # Grabbing data after 2012
    df <- raw_data[raw_data$Year >= 1972,]


    gerrymandering_sim <- function(year, state, match_threshold=.01){
      #' This function simulates gerrymandering for a given year and state 
      #' by comparing the actual Democratic vote percentage with 10,000        #' bootstrap samples to assess the number of Democratic seats. 
      #'
      #' @param year The year for which the simulation is performed.
      #' @param state The state for which the simulation is performed.
      #' @param match_threshold The threshold for a set of districts to 
      #' match the partisan of a state
      #' @return A plot showing the dist. of the number of Democratic seats.
      
      # Grabbing DEM % for NC
      dem_votes <- sum(df$Dem.Votes[df$Year == year & df$State == state])
      gop_votes <- sum(df$GOP.Votes[df$Year == year & df$State == state])
      dem_perc <- dem_votes/(dem_votes+gop_votes)
      
      # Grabs number Democrat in NC in 2012
      actual_dem_seats <- sum(df$Year == year & 
        (df$State == state & df$Party == "D"))
      
      # Number of districts in each state
      num_districts <- sum(df$Year == year & df$State == state)
      
      # Begin Simulation
      simulated_seats <- c()
      
      for (i in 1:10000){
        # Bootstrap Sampling n number of  
        sampled_data <- df[sample(1:nrow(df),num_districts),]
        
        # Grabbing Dem %
        sim_dem_votes <- sum(sampled_data$Dem.Votes)
        sim_gop_votes <- sum(sampled_data$GOP.Votes)
        sim_dem_perc <- sim_dem_votes/(sim_dem_votes+sim_gop_votes)
        
        # Compare to actual percentage
        if (abs(sim_dem_perc-dem_perc) < match_threshold){
          # Add the number of D to r2
          simulated_seats <- c(simulated_seats,sum(sampled_data$Party=="D"))
        }
      }
      
      # Plotting Simulation Results
      ggplot() + geom_bar(aes(x=simulated_seats)) + 
        geom_rect(aes(xmin=actual_dem_seats-.5,
                      xmax=actual_dem_seats+.5,
                      ymin=0,ymax=14), fill="red") +
        scale_x_continuous(breaks=c(3:11)) + 
        theme_minimal() + 
        labs(x="Number Seats",y="Count")
    }


    gerrymandering_sim(2012, 'NC')
    gerrymandering_sim(2016, 'PA')
    ```

3.  Your goal is to do a quality check on the data. There are two types of checks you should think about: single-source checks or multiple-source checks. A single-source problem can relate to the attributes (columns), records (rows), or the data source. A multiple-source problem relates to how the two data sets relate to each other. To get started, look over the data codebook and then brainstorm some things you plan to check in the data. Your code and report should use the same reproducible guidelines above.
